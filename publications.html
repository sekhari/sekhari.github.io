<h2> Preprints/Working Papers </h2>
<ul> 

  <li> 
      <p> 
          <strong> GaussMark: A Practical Approach for Structural Watermarking of Language Models</strong> 
          <br> Adam Block, Alexander Rakhlin, and Ayush Sekhari  
      </p>
  </li> 

  <li> 
      <p> 
          <strong> The Space Complexity of Learning-Unlearning Schemes </strong> 
          <br> Yeshwanth Cherapanamjeri, Sumegha Garg, Nived Rajaraman, Ayush Sekhari<sup style="color: black;">@</sup>, and Abhishek Shetty 
      </p>
  </li> 

  <li> 
      <p> 
          <strong> System Aware Unlearning Algorithms: Use Lesser, Forget Faster </strong> 
          <br> Linda Lu, Ayush Sekhari<sup style="color: black;">@</sup>, and Karthik Sridharan 
          <br>  
      </p>
  </li>


  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2410.08074" target="_blank">Unstable Unlearning: The Hidden Risk of Concept Resurgence in Diffusion Models</a> </strong> &nbsp; <a class="text-button3">Arxiv</a>
          <br> Vinith M. Suriyakumar*, Rohan Alur*, Ayush Sekhari, Manish Raghavan, Ashia C. Wilson
          <br>               <a href="javascript:toggleblock('dash_abs28')">Abstract</a> |
          <a href="https://arxiv.org/abs/2410.08074">ArXiv</a> 
          <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->
  
          <p align="justify"> <i style="display: none;" id="dash_abs28">Text-to-image diffusion models rely on massive, web-scale datasets. Training them from scratch is computationally expensive, and as a result, developers often prefer to make incremental updates to existing models. These updates often compose fine-tuning steps (to learn new concepts or improve model performance) with "unlearning" steps (to "forget" existing concepts, such as copyrighted works or explicit content). In this work, we demonstrate a critical and previously unknown vulnerability that arises in this paradigm: even under benign, non-adversarial conditions, fine-tuning a text-to-image diffusion model on seemingly unrelated images can cause it to "relearn" concepts that were previously "unlearned." We comprehensively investigate the causes and scope of this phenomenon, which we term concept resurgence, by performing a series of experiments which compose "mass concept erasure" (the current state of the art for unlearning in text-to-image diffusion models (Lu et al., 2024)) with subsequent fine-tuning of Stable Diffusion v1.4. Our findings underscore the fragility of composing incremental model updates, and raise serious new concerns about current approaches to ensuring the safety and alignment of text-to-image diffusion models.
         </i></p>  

          <!-- <br> Preprint arXiv:2410.08074.--> 
      </p>
  </li>

  <li> 
      <p> 
          <strong> <a href="https://arxiv.org/abs/2407.04264" target="_blank">Langevin Dynamics: A Unified Perspective on Optimization via Lyapunov Potentials</a> </strong> &nbsp; <a class="text-button3">Arxiv</a>
          <br> August Y. Chen, Ayush Sekhari<sup style="color: black;">@</sup>, and Karthik Sridharan   
          <br> Preliminary version at OPT for ML 2024 Workshop at NeurIPS 2024.
          <br>               <a href="javascript:toggleblock('dash_abs27')">Abstract</a> |
          <a href="https://arxiv.org/abs/2407.04264">ArXiv</a> 
          <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->
  
          <p align="justify"> <i style="display: none;" id="dash_abs27">We study the problem of non-convex optimization using Stochastic Gradient Langevin Dynamics (SGLD). SGLD is a natural and popular variation of stochastic gradient descent where at each step, appropriately scaled Gaussian noise is added. To our knowledge, the only strategy for showing global convergence of SGLD on the loss function is to show that SGLD can sample from a stationary distribution which assigns larger mass when the function is small (the Gibbs measure), and then to convert these guarantees to optimization results.
            We employ a new strategy to analyze the convergence of SGLD to global minima, based on Lyapunov potentials and optimization. We convert the same mild conditions from previous works on SGLD into geometric properties based on Lyapunov potentials. This adapts well to the case with a stochastic gradient oracle, which is natural for machine learning applications where one wants to minimize population loss but only has access to stochastic gradients via minibatch training samples. Here we provide 1) improved rates in the setting of previous works studying SGLD for optimization, 2) the first finite gradient complexity guarantee for SGLD where the function is Lipschitz and the Gibbs measure defined by the function satisfies a Poincaré Inequality, and 3) prove if continuous-time Langevin Dynamics succeeds for optimization, then discrete-time SGLD succeeds under mild regularity assumptions.
         </i></p>  
      </p>
  </li> 
</ul> 


<h2> Conference Publications </h2>
<ul> 


    <li> 
        <p> 
            <strong> <a href="https://arxiv.org/abs/2406.17216" target="_blank">Machine Unlearning Fails to Remove Data Poisoning Attacks</a> </strong> &nbsp; <a class="text-button3">ICLR 2025</a>
            <br> Martin Pawelczyk*, Jimmy Z. Di*, Yiwei Lu, Gautam Kamath*, Ayush Sekhari* (equal advisory contribution), and Seth Neel* 
            <br> Preliminary version accepted as <font color="red">spotlight presentation</font> at Generative AI and Law Workshop (GenLaw'24) at ICML 2024.
            <br>               <a href="javascript:toggleblock('dash_abs26')">Abstract</a> |
            <a href="https://arxiv.org/abs/2406.17216">ArXiv</a> 
            <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

            <p align="justify"> <i style="display: none;" id="dash_abs26">We revisit the efficacy of several practical methods for approximate machine unlearning developed for large-scale deep learning. In addition to complying with data deletion requests, one often-cited potential application for unlearning methods is to remove the effects of training on poisoned data. We experimentally demonstrate that, while existing unlearning methods have been demonstrated to be effective in a number of evaluation settings (e.g., alleviating membership inference attacks), they fail to remove the effects of data poisoning, across a variety of types of poisoning attacks (indiscriminate, targeted, and a newly-introduced Gaussian poisoning attack) and models (image classifiers and LLMs); even when granted a relatively large compute budget. In order to precisely characterize unlearning efficacy, we introduce new evaluation metrics for unlearning based on data poisoning. Our results suggest that a broader perspective, including a wider variety of evaluations, is required to avoid a false sense of confidence in machine unlearning procedures for deep learning without provable guarantees. Moreover, while unlearning methods show some signs of being useful to efficiently remove poisoned datapoints without having to retrain, our work suggests that these methods are not yet "ready for prime time", and currently provide limited benefit over retraining.
           </i></p>  
        </p>
    </li>

    <li> 
        <p> 
            <strong> <a href="https://arxiv.org/abs/2406.11810" target="_blank">Computationally Efficient RL under Linear Bellman Completeness for Deterministic Dynamics</a> </strong> &nbsp; <a class="text-button3">ICLR 2025</a>
            <br> Runzhe Wu*, Ayush Sekhari* (equal contribution), Akshay Krishnamurthy, Wen Sun 
            <br>               <a href="javascript:toggleblock('dash_abs25')">Abstract</a> |
            <a href="https://arxiv.org/abs/2406.11810">ArXiv</a>  | 
            <a href="https://www.youtube.com/watch?v=TFdHS-o5ss8&ab_channel=RLtheoryseminars">Talk (by Runzhe Wu)</a> 
            <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

            <p align="justify"> <i style="display: none;" id="dash_abs25">We study computationally and statistically efficient Reinforcement Learning algorithms for the linear Bellman Complete setting, a setting that uses linear function approximation to capture value functions and unifies existing models like linear Markov Decision Processes (MDP) and Linear Quadratic Regulators (LQR). While it is known from the prior works that this setting is statistically tractable, it remained open whether a computationally efficient algorithm exists. Our work provides a computationally efficient algorithm for the linear Bellman complete setting that works for MDPs with large action spaces, random initial states, and random rewards but relies on the underlying dynamics to be deterministic. Our approach is based on randomization: we inject random noise into least square regression problems to perform optimistic value iteration. Our key technical contribution is to carefully design the noise to only act in the null space of the training data to ensure optimism while circumventing a subtle error amplification issue.
           </i></p>  

        </p>
    </li>

    <li> 
        <p> 
            <strong> <a href="https://arxiv.org/abs/2403.17091" target="_blank">Offline Reinforcement Learning: Role of State Aggregation and Trajectory Data</a> </strong>  
          
            &nbsp; <a class="text-button3">COLT 2024</a>
            <br> Zeyu Jia, Alexander Rakhlin, Ayush Sekhari<sup style="color: black;">@</sup>, and Chen-Yu Wei  

            <br> 
              <a href="javascript:toggleblock('dash_abs24')">Abstract</a> |
              <a href="https://arxiv.org/abs/2403.17091">ArXiv</a>  | 
              <a href="https://www.youtube.com/watch?v=kQ9v0sy8Cao&ab_channel=RLtheoryseminars">Talk (by Zeyu Jia)</a> 
              <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

              <p align="justify"> <i style="display: none;" id="dash_abs24">We revisit the problem of offline reinforcement learning with value function realizability but without Bellman completeness. Previous work by Xie and Jiang (2021) and Foster et al. (2022) left open the question whether a bounded concentrability coefficient along with trajectory-based offline data admits a polynomial sample complexity. In this work, we provide a negative answer to this question for the task of offline policy evaluation. In addition to addressing this question, we provide a rather complete picture for offline policy evaluation with only value function realizability. Our primary findings are threefold: 1) The sample complexity of offline policy evaluation is governed by the concentrability coefficient in an aggregated Markov Transition Model jointly determined by the function class and the offline data distribution, rather than that in the original MDP. This unifies and generalizes the ideas of Xie and Jiang (2021) and Foster et al. (2022), 2) The concentrability coefficient in the aggregated Markov Transition Model may grow exponentially with the horizon length, even when the concentrability coefficient in the original MDP is small and the offline data is admissible (i.e., the data distribution equals the occupancy measure of some policy), 3) Under value function realizability, there is a generic reduction that can convert any hard instance with admissible data to a hard instance with trajectory data, implying that trajectory data offers no extra benefits over admissible data. These three pieces jointly resolve the open problem, though each of them could be of independent interest.
            </i></p>                      
        </p> 
    </li> 



  <li> 
    <p> 
        <strong> <a href="https://srinathm1359.github.io/random-latent-exploration/" target="_blank">Random Latent Exploration for Deep Reinforcement Learning</a> </strong>  &nbsp; <a class="text-button3">ICML 2024</a> 
        <br> Srinath Mahankali, Zhang-Wei Hong, Ayush Sekhari, Alexander Rakhlin, Pulkit Agrawal
        <br> 
        <a href="javascript:toggleblock('dash_abs23')">Abstract</a> |
        <a href="https://arxiv.org/abs/2407.13755">ArXiv</a>  | 
        <a href="https://srinathm1359.github.io/random-latent-exploration/"> Project Website </a> | 
        <a class="fa fa-github w3-hover-opacity" href="https://github.com/Improbable-AI/random-latent-exploration"></i></a> 

        <p align="justify"> <i style="display: none;" id="dash_abs23">The ability to efficiently explore high-dimensional state spaces is essential for the practical success of deep Reinforcement Learning (RL). This paper introduces a new exploration technique called Random Latent Exploration (RLE), that combines the strengths of bonus-based and noise-based (two popular approaches for effective exploration in deep RL) exploration strategies. RLE leverages the idea of perturbing rewards by adding structured random rewards to the original task rewards in certain (random) states of the environment, to encourage the agent to explore the environment during training. RLE is straightforward to implement and performs well in practice. To demonstrate the practical effectiveness of RLE, we evaluate it on the challenging Atari and IsaacGym benchmarks and show that RLE exhibits higher overall scores across all the tasks than other approaches.
      </i></p>  
    </p>
</li> 



<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2401.09681" target="_blank">Harnessing Density Ratios for Online Reinforcement Learning</a> </strong> &nbsp; <a class="text-button3">ICLR 2024</a>           <a class="text-button">Spotlight</a>

        <br> Philip Amortila, Dylan J. Foster, Nan Jiang, Ayush Sekhari<sup style="color: black;">@</sup>, and Tengyang Xie  
        <br>               <a href="javascript:toggleblock('dash_abs22')">Abstract</a> |
        <a href="https://arxiv.org/abs/2401.09681">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        <p align="justify"> <i style="display: none;" id="dash_abs22">The theories of offline and online reinforcement learning, despite having evolved in parallel, have begun to show signs of the possibility for a unification, with algorithms and analysis techniques for one setting often having natural counterparts in the other. However, the notion of density ratio modeling, an emerging paradigm in offline RL, has been largely absent from online RL, perhaps for good reason: the very existence and boundedness of density ratios relies on access to an exploratory dataset with good coverage, but the core challenge in online RL is to collect such a dataset without having one to start. In this work we show -- perhaps surprisingly -- that density ratio-based algorithms have online counterparts. Assuming only the existence of an exploratory distribution with good coverage, a structural condition known as coverability (Xie et al., 2023), we give a new algorithm (GLOW) that uses density ratio realizability and value function realizability to perform sample-efficient online exploration. GLOW addresses unbounded density ratios via careful use of truncation, and combines this with optimism to guide exploration. GLOW is computationally inefficient; we complement it with a more efficient counterpart, HyGLOW, for the Hybrid RL setting (Song et al., 2022) wherein online RL is augmented with additional offline data. HyGLOW is derived as a special case of a more general meta-algorithm that provides a provable black-box reduction from hybrid RL to offline RL, which may be of independent interest.
       </i></p>  
    </p>
</li> 

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2311.08384" target="_blank">Offline Data Enhanced On-Policy Policy Gradient with Provable Guarantees</a> </strong> &nbsp; <a class="text-button3">ICLR 2024</a>
        <br> Yifei Zhou*, Ayush Sekhari* (equal contribution), Yuda Song, and Wen Sun
        <br>               <a href="javascript:toggleblock('dash_abs21')">Abstract</a> |
        <a href="https://arxiv.org/abs/2311.08384">ArXiv</a> | 
        <a href=""https://github.com/YifeiZhou02/HNPG"><i class="fa fa-github w3-hover-opacity"></i></a> | 
        <a href="https://youtu.be/WmO8Lp4DfmU">Talk (1 hr)</a>
    
        <p align="justify"> <i style="display: none;" id="dash_abs21">Hybrid RL is the setting where an RL agent has access to both offline data and online data by interacting with the real-world environment. In this work, we propose a new hybrid RL algorithm that combines an on-policy actor-critic method with offline data. On-policy methods such as policy gradient and natural policy gradient (NPG) have shown to be more robust to model misspecification, though sometimes it may not be as sample efficient as methods that rely on off-policy learning. On the other hand, offline methods that depend on off-policy training often require strong assumptions in theory and are less stable to train in practice. Our new approach integrates a procedure of off-policy training on the offline data into an on-policy NPG framework. We show that our approach, in theory, can obtain a best-of-both-worlds type of result -- it achieves the state-of-art theoretical guarantees of offline RL when offline RL-specific assumptions hold, while at the same time maintaining the theoretical guarantees of on-policy NPG regardless of the offline RL assumptions' validity. Experimentally, in challenging rich-observation environments, we show that our approach outperforms a state-of-the-art hybrid RL baseline which only relies on off-policy policy optimization, demonstrating the empirical benefit of combining on-policy and off-policy learning. Our code is publicly available. 
       </i></p>  

    </p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/pdf/2310.06113.pdf" target="_blank">When is Agnostic Reinforcement Learning Statistically Tractable?</a> </strong> &nbsp; <a class="text-button3">NeurIPS 2023</a>
        <br> Zeyu Jia, Gene Li, Alexander Rakhlin, Ayush Sekhari<sup style="color: black;">@</sup>, and Nathan Srebro 
        <br>               <a href="javascript:toggleblock('dash_abs20')">Abstract</a> |
        <a href="https://arxiv.org/abs/2310.06113">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        <p align="justify"> <i style="display: none;" id="dash_abs20">We study the problem of agnostic PAC reinforcement learning (RL): given a policy class Π, how many rounds of interaction with an unknown MDP (with a potentially large state and action space) are required to learn an ϵ-suboptimal policy with respect to Π? Towards that end, we introduce a new complexity measure, called the \emph{spanning capacity}, that depends solely on the set Π and is independent of the MDP dynamics. With a generative model, we show that for any policy class Π, bounded spanning capacity characterizes PAC learnability. However, for online RL, the situation is more subtle. We show there exists a policy class Π with a bounded spanning capacity that requires a superpolynomial number of samples to learn. This reveals a surprising separation for agnostic learnability between generative access and online access models (as well as between deterministic/stochastic MDPs under online access). On the positive side, we identify an additional \emph{sunflower} structure, which in conjunction with bounded spanning capacity enables statistically efficient online RL via a new algorithm called POPLER, which takes inspiration from classical importance sampling methods as well as techniques for reachable-state identification and policy evaluation in reward-free exploration.
       </i></p>  

    </p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2307.04998" target="_blank">Selective Sampling and Imitation Learning via Online Regression</a> </strong> &nbsp; <a class="text-button3">NeurIPS 2023</a>
        <br> Ayush Sekhari<sup style="color: black;">@</sup>, Karthik Sridharan, Wen Sun, and Runzhe Wu 
        <br> Short version also appeared at <a href="https://interactive-learning-implicit-feedback.github.io/" target="_blank">Interactive Learning with Implicit Human Feedback workshop</a> at ICML 2023.  
        <br>               <a href="javascript:toggleblock('dash_abs19')">Abstract</a> |
        <a href="https://arxiv.org/abs/2307.04998">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        <p align="justify"> <i style="display: none;" id="dash_abs19">We consider the problem of Imitation Learning (IL) by actively querying noisy expert for feedback. While imitation learning has been empirically successful, much of prior work assumes access to noiseless expert feedback which is not practical in many applications. In fact, when one only has access to noisy expert feedback, algorithms that rely on purely offline data (non-interactive IL) can be shown to need a prohibitively large number of samples to be successful. In contrast, in this work, we provide an interactive algorithm for IL that uses selective sampling to actively query the noisy expert for feedback. Our contributions are twofold: First, we provide a new selective sampling algorithm that works with general function classes and multiple actions, and obtains the best-known bounds for the regret and the number of queries. Next, we extend this analysis to the problem of IL with noisy expert feedback and provide a new IL algorithm that makes limited queries.
            Our algorithm for selective sampling leverages function approximation, and relies on an online regression oracle w.r.t.~the given model class to predict actions, and to decide whether to query the expert for its label. On the theoretical side, the regret bound of our algorithm is upper bounded by the regret of the online regression oracle, while the query complexity additionally depends on the eluder dimension of the model class. We complement this with a lower bound that demonstrates that our results are tight. We extend our selective sampling algorithm for IL with general function approximation and provide bounds on both the regret and the number of queries made to the noisy expert. A key novelty here is that our regret and query complexity bounds only depend on the number of times the optimal policy (and not the noisy expert, or the learner) go to states that have a small margin.
       </i></p>  

    </p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2307.12926" target="_blank">Contextual Bandits and Imitation Learning via Preference-Based Active Queries</a> </strong> &nbsp; <a class="text-button3">NeurIPS 2023</a>
        <br> Ayush Sekhari<sup style="color: black;">@</sup>, Karthik Sridharan, Wen Sun, and Runzhe Wu 
        <br> Short version also appeared at <a href="https://interactive-learning-implicit-feedback.github.io/" target="_blank">Interactive Learning with Implicit Human Feedback workshop</a>, and <a href="https://sites.google.com/view/mfpl-icml-2023" target="_blank">The Many Facets of Preference-Based Learning workshop</a> at ICML 2023.  

        <br>               <a href="javascript:toggleblock('dash_abs18')">Abstract</a> |
        <a href="https://arxiv.org/abs/2307.12926">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        <p align="justify"> <i style="display: none;" id="dash_abs18">We consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action's reward. Instead, the learner can actively query an expert at each round to compare two actions and receive noisy preference feedback. The learner's objective is two-fold: to minimize the regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert's preference model under appropriate link functions, and provide an algorithm that leverages an online regression oracle with respect to this function class for choosing its actions and deciding when to query. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as O(min{T‾‾√,d/Δ}), where T represents the number of interactions, d represents the eluder dimension of the function class, and Δ represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of Δ, and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only O(min{T,d2/Δ2}) queries to the expert. We then extend our algorithm to the imitation learning setting, where the learning agent engages with an unknown environment in episodes of length H each, and provide similar guarantees for regret and query complexity. Interestingly, our algorithm for imitation learning can even learn to outperform the underlying expert, when it is suboptimal, highlighting a practical benefit of preference-based feedback in imitation learning.
       </i></p>  

    </p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2212.10717" target="_blank">Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks</a> </strong> &nbsp; <a class="text-button3">NeurIPS 2023</a>
        <br> Jimmy Z. Di*, Jack Douglas*, Jayadev Acharya, Gautam Kamath, and Ayush Sekhari 

        <br>               <a href="javascript:toggleblock('dash_abs17')">Abstract</a> |
        <a href="https://arxiv.org/abs/2212.10717">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        <p align="justify"> <i style="display: none;" id="dash_abs17">We introduce camouflaged data poisoning attacks, a new attack vector that arises in the context of machine unlearning and other settings when model retraining may be induced. An adversary first adds a few carefully crafted points to the training dataset such that the impact on the model's predictions is minimal. The adversary subsequently triggers a request to remove a subset of the introduced points at which point the attack is unleashed and the model's predictions are negatively affected. In particular, we consider clean-label targeted attacks (in which the goal is to cause the model to misclassify a specific test point) on datasets including CIFAR-10, Imagenette, and Imagewoof. This attack is realized by constructing camouflage datapoints that mask the effect of a poisoned dataset.
       </i></p>  

    </p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2211.14250" target="_blank">Model-Free Reinforcement Learning with the Decision-Estimation Coefficient</a> </strong> &nbsp; <a class="text-button3">NeurIPS 2023</a>
        <br> Dylan J. Foster, Noah Golowich, Jian Qian, Alexander Rakhlin, and Ayush Sekhari<sup style="color: black;">@</sup> 
        <br>               <a href="javascript:toggleblock('dash_abs16')">Abstract</a> |
        <a href="https://arxiv.org/abs/2211.14250">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        <p align="justify"> <i style="display: none;" id="dash_abs16">We consider the problem of interactive decision making, encompassing structured bandits and reinforcement learning with general function approximation. Recently, Foster et al. (2021) introduced the Decision-Estimation Coefficient, a measure of statistical complexity that lower bounds the optimal regret for interactive decision making, as well as a meta-algorithm, Estimation-to-Decisions, which achieves upper bounds in terms of the same quantity. Estimation-to-Decisions is a reduction, which lifts algorithms for (supervised) online estimation into algorithms for decision making. In this paper, we show that by combining Estimation-to-Decisions with a specialized form of optimistic estimation introduced by Zhang (2022), it is possible to obtain guarantees that improve upon those of Foster et al. (2021) by accommodating more lenient notions of estimation error. We use this approach to derive regret bounds for model-free reinforcement learning with value function approximation, and give structural results showing when it can and cannot help more generally.
       </i></p>  
    </p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2306.15744" target="_blank">Ticketed Learning-Unlearning Schemes</a> </strong> &nbsp; <a class="text-button3">COLT 2023</a>
        <br> Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Ayush Sekhari<sup style="color: black;">@</sup>, and Chiyuan Zhang
        <br> Short version at Symposium on the Foundations of Responsible Computing, FORC 2023.  
        <br>               <a href="javascript:toggleblock('dash_abs15')">Abstract</a> |
        <a href="https://arxiv.org/abs/2306.15744">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        <p align="justify"> <i style="display: none;" id="dash_abs15">We consider the learning--unlearning paradigm defined as follows. First given a dataset, the goal is to learn a good predictor, such as one minimizing a certain loss. Subsequently, given any subset of examples that wish to be unlearnt, the goal is to learn, without the knowledge of the original training dataset, a good predictor that is identical to the predictor that would have been produced when learning from scratch on the surviving examples.
            We propose a new ticketed model for learning--unlearning wherein the learning algorithm can send back additional information in the form of a small-sized (encrypted) ``ticket'' to each participating training example, in addition to retaining a small amount of ``central'' information for later. Subsequently, the examples that wish to be unlearnt present their tickets to the unlearning algorithm, which additionally uses the central information to return a new predictor. We provide space-efficient ticketed learning--unlearning schemes for a broad family of concept classes, including thresholds, parities, intersection-closed classes, among others.
            En route, we introduce the count-to-zero problem, where during unlearning, the goal is to simply know if there are any examples that survived. We give a ticketed learning--unlearning scheme for this problem that relies on the construction of Sperner families with certain properties, which might be of independent interest.
       </i></p>  

    </p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2206.12081" target="_blank">Computationally Efficient PAC RL in POMDPs with Latent Determinism and Conditional Embeddings</a> </strong> &nbsp; <a class="text-button3">ICML 2023</a>
        <br> Masatoshi Uehara, Ayush Sekhari, Jason D. Lee, Nathan Kallus, and Wen Sun 
        <br>               <a href="javascript:toggleblock('dash_abs14')">Abstract</a> |
        <a href="https://arxiv.org/abs/2206.12081">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        <p align="justify"> <i style="display: none;" id="dash_abs14">We study reinforcement learning with function approximation for large-scale Partially Observable Markov Decision Processes (POMDPs) where the state space and observation space are large or even continuous. Particularly, we consider Hilbert space embeddings of POMDP where the feature of latent states and the feature of observations admit a conditional Hilbert space embedding of the observation emission process, and the latent state transition is deterministic. Under the function approximation setup where the optimal latent state-action Q-function is linear in the state feature, and the optimal Q-function has a gap in actions, we provide a \emph{computationally and statistically efficient} algorithm for finding the \emph{exact optimal} policy. We show our algorithm's computational and statistical complexities scale polynomially with respect to the horizon and the intrinsic dimension of the feature on the observation space. Furthermore, we show both the deterministic latent transitions and gap assumptions are necessary to avoid statistical complexity exponential in horizon or dimension. Since our guarantee does not have an explicit dependence on the size of the state and observation spaces, our algorithm provably scales to large-scale POMDPs.
       </i></p>  

    </p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2210.06718" target="_blank">Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient</a> </strong> &nbsp; <a class="text-button3">ICLR 2023</a>
        <br> Yuda Song*, Yifei Zhou*, Ayush Sekhari, J. Andrew Bagnell, Akshay Krishnamurthy, and Wen Sun 
        <br>               <a href="javascript:toggleblock('dash_abs13')">Abstract</a> |
        <a href="https://arxiv.org/abs/2210.06718">ArXiv</a> | 
        <a href="https://github.com/yudasong/HyQ"><i class="fa fa-github w3-hover-opacity"></i></a> 

        <p align="justify"> <i style="display: none;" id="dash_abs13">We consider a hybrid reinforcement learning setting (Hybrid RL), in which an agent has access to an offline dataset and the ability to collect experience via real-world online interaction. The framework mitigates the challenges that arise in both pure offline and online RL settings, allowing for the design of simple and highly effective algorithms, in both theory and practice. We demonstrate these advantages by adapting the classical Q learning/iteration algorithm to the hybrid setting, which we call Hybrid Q-Learning or Hy-Q. In our theoretical results, we prove that the algorithm is both computationally and statistically efficient whenever the offline dataset supports a high-quality policy and the environment has bounded bilinear rank. Notably, we require no assumptions on the coverage provided by the initial distribution, in contrast with guarantees for policy gradient/iteration methods. In our experimental results, we show that Hy-Q with neural network function approximation outperforms state-of-the-art online, offline, and hybrid RL baselines on challenging benchmarks, including Montezuma's Revenge.
       </i></p>  

    </p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2210.06705" target="_blank">From Gradient Flow on Population Loss to Learning with Stochastic Gradient Descent</a> </strong> &nbsp; <a class="text-button3">NeurIPS 2022</a>
        <br> Satyen Kale, Jason D. Lee, Chris De Sa, Ayush Sekhari<sup style="color: black;">@</sup>, and Karthik Sridharan. 
        <br>               <a href="javascript:toggleblock('dash_abs12')">Abstract</a> |
        <a href="https://arxiv.org/abs/2210.06705">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        <p align="justify"> <i style="display: none;" id="dash_abs12">Stochastic Gradient Descent (SGD) has been the method of choice for learning large-scale non-convex models. While a general analysis of when SGD works has been elusive, there has been a lot of recent progress in understanding the convergence of Gradient Flow (GF) on the population loss, partly due to the simplicity that a continuous-time analysis buys us. An overarching theme of our paper is providing general conditions under which SGD converges, assuming that GF on the population loss converges. Our main tool to establish this connection is a general converse Lyapunov like theorem, which implies the existence of a Lyapunov potential under mild assumptions on the rates of convergence of GF. In fact, using these potentials, we show a one-to-one correspondence between rates of convergence of GF and geometrical properties of the underlying objective. When these potentials further satisfy certain self-bounding properties, we show that they can be used to provide a convergence guarantee for Gradient Descent (GD) and SGD (even when the paths of GF and GD/SGD are quite far apart). It turns out that these self-bounding assumptions are in a sense also necessary for GD/SGD to work. Using our framework, we provide a unified analysis for GD/SGD not only for classical settings like convex losses, or objectives that satisfy PL / KL properties, but also for more complex problems including Phase Retrieval and Matrix sq-root, and extending the results in the recent work of Chatterjee 2022.
       </i></p>  

    </p>
</li> 

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2206.13063" target="_blank">On the Complexity of Adversarial Decision Making</a> </strong> &nbsp; <a class="text-button3">NeurIPS 2022</a>          <a class="text-button">Oral</a>

        <br> Dylan J. Foster, Alexander Rakhlin, Ayush Sekhari<sup style="color: black;">@</sup>, and Karthik Sridharan 
        <!-- <br> <font color="red">(Oral Presentation)</font> --> 
        <br>               <a href="javascript:toggleblock('dash_abs11')">Abstract</a> |
        <a href="https://arxiv.org/abs/2206.13063">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        <p align="justify"> <i style="display: none;" id="dash_abs11">A central problem in online learning and decision making -- from bandits to reinforcement learning -- is to understand what modeling assumptions lead to sample-efficient learning guarantees. We consider a general adversarial decision making framework that encompasses (structured) bandit problems with adversarial rewards and reinforcement learning problems with adversarial dynamics. Our main result is to show -- via new upper and lower bounds -- that the Decision-Estimation Coefficient, a complexity measure introduced by Foster et al. in the stochastic counterpart to our setting, is necessary and sufficient to obtain low regret for adversarial decision making. However, compared to the stochastic setting, one must apply the Decision-Estimation Coefficient to the convex hull of the class of models (or, hypotheses) under consideration. This establishes that the price of accommodating adversarial rewards or dynamics is governed by the behavior of the model class under convexification, and recovers a number of existing results -- both positive and negative. En route to obtaining these guarantees, we provide new structural results that connect the Decision-Estimation Coefficient to variants of other well-known complexity measures, including the Information Ratio of Russo and Van Roy and the Exploration-by-Optimization objective of Lattimore and György.
       </i></p>  

    </p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2206.12020" target="_blank">Provably Efficient Reinforcement Learning in Partially Observable Dynamical Systems</a> </strong> &nbsp; <a class="text-button3">NeurIPS 2022</a>
        <br> Masatoshi Uehara, Ayush Sekhari, Jason D. Lee, Nathan Kallus, and Wen Sun 
        <!--<br> <strong>NeurIPS 2022.</strong> --> 
        <br>               <a href="javascript:toggleblock('dash_abs10')">Abstract</a> |
        <a href="https://arxiv.org/abs/2206.12020">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        <p align="justify"> <i style="display: none;" id="dash_abs10">We study Reinforcement Learning for partially observable dynamical systems using function approximation. We propose a new \textit{Partially Observable Bilinear Actor-Critic framework}, that is general enough to include models such as observable tabular Partially Observable Markov Decision Processes (POMDPs), observable Linear-Quadratic-Gaussian (LQG), Predictive State Representations (PSRs), as well as a newly introduced model Hilbert Space Embeddings of POMDPs and observable POMDPs with latent low-rank transition. Under this framework, we propose an actor-critic style algorithm that is capable of performing agnostic policy learning. Given a policy class that consists of memory based policies (that look at a fixed-length window of recent observations), and a value function class that consists of functions taking both memory and future observations as inputs, our algorithm learns to compete against the best memory-based policy in the given policy class. For certain examples such as undercomplete observable tabular POMDPs, observable LQGs and observable POMDPs with latent low-rank transition, by implicitly leveraging their special properties, our algorithm is even capable of competing against the globally optimal policy without paying an exponential dependence on the horizon in its sample complexity.
       </i></p>  
    </p>
</li> 

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2206.09421" target="_blank">Guarantees for Epsilon-Greedy Reinforcement Learning with Function Approximation</a> </strong> &nbsp; <a class="text-button3">ICML 2022</a>
        <br> Christoph Dann, Yishay Mansour, Mehryar Mohri, Ayush Sekhari<sup style="color: black;">@</sup>, and Karthik Sridharan 
        <br> Short version at <a href='https://rldm.org/'>RLDM 2022</a> - Reinforcement Learning and Decision Making conference.
        <br>               <a href="javascript:toggleblock('dash_abs9')">Abstract</a> |
        <a href="https://arxiv.org/abs/2206.09421">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        <p align="justify"> <i style="display: none;" id="dash_abs9">Myopic exploration policies such as epsilon-greedy, softmax, or Gaussian noise fail to explore efficiently in some reinforcement learning tasks and yet, they perform well in many others. In fact, in practice, they are often selected as the top choices, due to their simplicity. But, for what tasks do such policies succeed? Can we give theoretical guarantees for their favorable performance? These crucial questions have been scarcely investigated, despite the prominent practical importance of these policies. This paper presents a theoretical analysis of such policies and provides the first regret and sample-complexity bounds for reinforcement learning with myopic exploration. Our results apply to value-function-based algorithms in episodic MDPs with bounded Bellman Eluder dimension. We propose a new complexity measure called myopic exploration gap, denoted by alpha, that captures a structural property of the MDP, the exploration policy and the given value function class. We show that the sample-complexity of myopic exploration scales quadratically with the inverse of this quantity, 1 / alpha^2. We further demonstrate through concrete examples that myopic exploration gap is indeed favorable in several tasks where myopic exploration succeeds, due to the corresponding dynamics and reward structure.
       </i></p>  
    </p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2107.05074" target="_blank">SGD: The Role of Implicit Regularization, Batch-size and Multiple Epochs</a> </strong> &nbsp; <a class="text-button3">NeurIPS 2021</a>
        <br> Satyen Kale, Ayush Sekhari<sup style="color: black;">@</sup>, and Karthik Sridharan 
        <br>               <a href="javascript:toggleblock('dash_abs8')">Abstract</a> |
        <a href="https://arxiv.org/abs/2107.05074">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        <p align="justify"> <i style="display: none;" id="dash_abs8">Multi-epoch, small-batch, Stochastic Gradient Descent (SGD) has been the method of choice for learning with large over-parameterized models. A popular theory for explaining why SGD works well in practice is that the algorithm has an implicit regularization that biases its output towards a good solution. Perhaps the theoretically most well understood learning setting for SGD is that of Stochastic Convex Optimization (SCO), where it is well known that SGD learns at a rate of O(1/n‾√), where n is the number of samples. In this paper, we consider the problem of SCO and explore the role of implicit regularization, batch size and multiple epochs for SGD. Our main contributions are threefold:
            (a) We show that for any regularizer, there is an SCO problem for which Regularized Empirical Risk Minimzation fails to learn. This automatically rules out any implicit regularization based explanation for the success of SGD.
            (b) We provide a separation between SGD and learning via Gradient Descent on empirical loss (GD) in terms of sample complexity. We show that there is an SCO problem such that GD with any step size and number of iterations can only learn at a suboptimal rate: at least Ω˜(1/n5/12).
            (c) We present a multi-epoch variant of SGD commonly used in practice. We prove that this algorithm is at least as good as single pass SGD in the worst case. However, for certain SCO problems, taking multiple passes over the dataset can significantly outperform single pass SGD.
            We extend our results to the general learning setting by showing a problem which is learnable for any data distribution, and for this problem, SGD is strictly better than RERM for any regularization function. We conclude by discussing the implications of our results for deep learning, and show a separation between SGD and ERM for two layer diagonal neural networks.     
        </i></p>  
    </p>
</li> 

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2106.11519" target="_blank">Agnostic Reinforcement Learning with Low-Rank MDPs and Rich Observations</a></strong> &nbsp; <a class="text-button3">NeurIPS 2021</a>          <a class="text-button">Spotlight</a>

        <br> Christoph Dann, Yishay Mansour, Mehryar Mohri, Ayush Sekhari<sup style="color: black;">@</sup>, and Karthik Sridharan 
        <br>               <a href="javascript:toggleblock('dash_abs7')">Abstract</a> |
        <a href="https://arxiv.org/abs/2106.11519">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        <p align="justify"> <i style="display: none;" id="dash_abs7">There have been many recent advances on provably efficient Reinforcement Learning (RL) in problems with rich observation spaces. However, all these works share a strong realizability assumption about the optimal value function of the true MDP. Such realizability assumptions are often too strong to hold in practice. In this work, we consider the more realistic setting of agnostic RL with rich observation spaces and a fixed class of policies Π that may not contain any near-optimal policy. We provide an algorithm for this setting whose error is bounded in terms of the rank d of the underlying MDP. Specifically, our algorithm enjoys a sample complexity bound of O((H4dK3dlog|Π|)/ϵ2) where H is the length of episodes, K is the number of actions and ϵ>0 is the desired sub-optimality. We also provide a nearly matching lower bound for this agnostic setting that shows that the exponential dependence on rank is unavoidable, without further assumptions.
       </i></p>  
        <!--<br> <font color="red">(Spotlight Presentation)</font>-->
    </p>
</li>

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2103.03279" target="_blank">Remember What You Want to Forget: Algorithms for Machine Unlearning</a></strong> &nbsp; <a class="text-button3">NeurIPS 2021</a>
        <br> Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh 
        <br> Short version at <a href='https://tpdp.journalprivacyconfidentiality.org/2021/'>TPDP 2021</a> - Theory and Practice of Differential Privacy.
        <br>               <a href="javascript:toggleblock('dash_abs6')">Abstract</a> |
        <a href="https://arxiv.org/abs/2103.03279">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        <p align="justify"> <i style="display: none;" id="dash_abs6">We study the problem of unlearning datapoints from a learnt model. The learner first receives a dataset S drawn i.i.d. from an unknown distribution, and outputs a model wˆ that performs well on unseen samples from the same distribution. However, at some point in the future, any training datapoint z∈S can request to be unlearned, thus prompting the learner to modify its output model while still ensuring the same accuracy guarantees. We initiate a rigorous study of generalization in machine unlearning, where the goal is to perform well on previously unseen datapoints. Our focus is on both computational and storage complexity.
            For the setting of convex losses, we provide an unlearning algorithm that can unlearn up to O(n/d1/4) samples, where d is the problem dimension. In comparison, in general, differentially private learning (which implies unlearning) only guarantees deletion of O(n/d1/2) samples. This demonstrates a novel separation between differential privacy and machine unlearning. 
              </i></p>  
    </p>
</li> 

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2106.03243" target="_blank">Neural Active Learning with Performance Guarantees</a></strong> &nbsp; <a class="text-button3">NeurIPS 2021</a>
        <br> Zhilei Wang, Pranjal Awasthi, Christoph Dann, Ayush Sekhari, and Claudio Gentile 
        <br>               <a href="javascript:toggleblock('dash_abs5')">Abstract</a> |
        <a href="https://arxiv.org/abs/2106.03243">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        <p align="justify"> <i style="display: none;" id="dash_abs5">We investigate the problem of active learning in the streaming setting in non-parametric regimes, where the labels are stochastically generated from a class of functions on which we make no assumptions whatsoever. We rely on recently proposed Neural Tangent Kernel (NTK) approximation tools to construct a suitable neural embedding that determines the feature space the algorithm operates on and the learned model computed atop. Since the shape of the label requesting threshold is tightly related to the complexity of the function to be learned, which is a-priori unknown, we also derive a version of the algorithm which is agnostic to any prior knowledge. This algorithm relies on a regret balancing scheme to solve the resulting online model selection problem, and is computationally efficient. We prove joint guarantees on the cumulative regret and number of requested labels which depend on the complexity of the labeling function at hand. In the linear case, these guarantees recover known minimax results of the generalization error as a function of the label complexity in a standard statistical learning setting.
       </i></p>  
    </p>
</li> 

<li> 
    <p> 
        <strong> <a href="https://arxiv.org/abs/2005.03789" target="_blank">Reinforcement Learning with Feedback Graphs</a></strong> &nbsp; <a class="text-button3">NeurIPS 2020</a>
        <br> Christoph Dann, Yishay Mansour, Mehryar Mohri, Ayush Sekhari<sup style="color: black;">@</sup>, and Karthik Sridharan 
        <br> Short version at <a href='https://sites.google.com/view/icml2018nonconvex/'>ICML 2020 Theoretical Foundations of RL workshop.</a> 
        <br>               <a href="javascript:toggleblock('dash_abs4')">Abstract</a> |
        <a href="https://arxiv.org/abs/2005.03789"> ArXiv</a> 

        <p align="justify"> <i style="display: none;" id="dash_abs4">We study episodic reinforcement learning in Markov decision processes when the agent receives additional feedback per step in the form of several transition observations. Such additional observations are available in a range of tasks through extended sensors or prior knowledge about the environment (e.g., when certain actions yield similar outcome). We formalize this setting using a feedback graph over state-action pairs and show that model-based algorithms can leverage the additional feedback for more sample-efficient learning. We give a regret bound that, ignoring logarithmic factors and lower-order terms, depends only on the size of the maximum acyclic subgraph of the feedback graph, in contrast with a polynomial dependency on the number of states and actions in the absence of a feedback graph. Finally, we highlight challenges when leveraging a small dominating set of the feedback graph as compared to the bandit setting and propose a new algorithm that can use knowledge of such a dominating set for more sample-efficient learning of a near-optimal policy.
       </i></p>  
    </p>
</li>

<li> 
    <p> 
        <strong> <a href='https://arxiv.org/abs/2006.13476' target="_blank">Second-Order Information in Non-Convex Stochastic Optimization: Power and Limitations</a></strong> &nbsp; <a class="text-button3">COLT 2020</a>
        <br> Yossi Arjevani, Yair Carmon, John C. Duchi, Dylan J. Foster, Ayush Sekhari<sup style="color: black;">@</sup>, and Karthik Sridharan 
        <br> Honorable mention for best talk award at <a href="https://www.nyas.org/events/2020/14th-annual-machine-learning-symposium/">NYAS ML symposium 2020</a>.
        <br>               <a href="javascript:toggleblock('dash_abs3')">Abstract</a> |
        <a href="https://arxiv.org/abs/2006.13476">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        <p align="justify"> <i style="display: none;" id="dash_abs3">We design an algorithm which finds an ϵ-approximate stationary point (with ‖∇F(x)‖≤ϵ) using O(ϵ−3) stochastic gradient and Hessian-vector products, matching guarantees that were previously available only under a stronger assumption of access to multiple queries with the same random seed. We prove a lower bound which establishes that this rate is optimal and---surprisingly---that it cannot be improved using stochastic pth order methods for any p≥2, even when the first p derivatives of the objective are Lipschitz. Together, these results characterize the complexity of non-convex stochastic optimization with second-order methods and beyond. Expanding our scope to the oracle complexity of finding (ϵ,γ)-approximate second-order stationary points, we establish nearly matching upper and lower bounds for stochastic second-order methods. Our lower bounds here are novel even in the noiseless case.

       </i></p>  
    </p>
</li>

<li> 
    <p> 
        <strong> <a href='https://arxiv.org/abs/1902.04686' target="_blank">The Complexity of Making the Gradient Small in Stochastic Convex Optimization</a></strong> &nbsp; <a class="text-button3">COLT 2019</a>           <a class="text-button">Best Student Paper Award</a>
 
        <br> Dylan J. Foster, Ayush Sekhari<sup style="color: black;">@</sup>, Ohad Shamir, Nathan Srebro, Karthik Sridharan, Blake Woodworth  
        <br>               <a href="javascript:toggleblock('dash_abs2')">Abstract</a> |
        <a href="https://arxiv.org/abs/1902.04686">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        <p align="justify"> <i style="display: none;" id="dash_abs2">We give nearly matching upper and lower bounds on the oracle complexity of finding ϵ-stationary points (‖∇F(x)‖≤ϵ) in stochastic convex optimization. We jointly analyze the oracle complexity in both the local stochastic oracle model and the global oracle (or, statistical learning) model. This allows us to decompose the complexity of finding near-stationary points into optimization complexity and sample complexity, and reveals some surprising differences between the complexity of stochastic optimization versus learning. Notably, we show that in the global oracle/statistical learning model, only logarithmic dependence on smoothness is required to find a near-stationary point, whereas polynomial dependence on smoothness is necessary in the local stochastic oracle model. In other words, the separation in complexity between the two models can be exponential, and that the folklore understanding that smoothness is required to find stationary points is only weakly true for statistical learning.
            Our upper bounds are based on extensions of a recent "recursive regularization" technique proposed by Allen-Zhu (2018). We show how to extend the technique to achieve near-optimal rates, and in particular show how to leverage the extra information available in the global oracle model. Our algorithm for the global model can be implemented efficiently through finite sum methods, and suggests an interesting new computational-statistical tradeoff. 
              </i></p>  
        <!-- <br> <strong> <font color="red">(Best Student Paper Award)</font> </strong>--> 
    </p>
</li>

<li> 
    <p> 
        <strong> <a href='https://arxiv.org/abs/1810.11059' target="_blank">Uniform Convergence of Gradients for Non-Convex Learning and Optimization</a></strong> &nbsp; <a class="text-button3">NeurIPS 2018</a>
        <br> Dylan J. Foster, Ayush Sekhari<sup style="color: black;">@</sup>, and Karthik Sridharan 
        <br> Short version at <a href='https://sites.google.com/view/icml2018nonconvex/' target="_blank">ICML 2018 Nonconvex Optimization workshop.</a>
        <br>               <a href="javascript:toggleblock('dash_abs1')">Abstract</a> |
        <a href="https://arxiv.org/abs/1810.11059">ArXiv</a> 
        <!-- <i class="fa fa-github w3-hover-opacity"></i></a> -->

        <p align="justify"> <i style="display: none;" id="dash_abs1">We investigate 1) the rate at which refined properties of the empirical risk---in particular, gradients---converge to their population counterparts in standard non-convex learning tasks, and 2) the consequences of this convergence for optimization. Our analysis follows the tradition of norm-based capacity control. We propose vector-valued Rademacher complexities as a simple, composable, and user-friendly tool to derive dimension-free uniform convergence bounds for gradients in non-convex learning problems. As an application of our techniques, we give a new analysis of batch gradient descent methods for non-convex generalized linear models and non-convex robust regression, showing how to use any algorithm that finds approximate stationary points to obtain optimal sample complexity, even when dimension is high or possibly infinite and multiple passes over the dataset are allowed.
            Moving to non-smooth models we show----in contrast to the smooth case---that even for a single ReLU it is not possible to obtain dimension-independent convergence rates for gradients in the worst case. On the positive side, it is still possible to obtain dimension-independent rates under a new type of distributional assumption.
       </i></p>  
    </p>
</li>


<div style="height: 20px;"></div>

<br> <br> <br> <br> 
